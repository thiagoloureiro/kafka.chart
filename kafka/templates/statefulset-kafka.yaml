apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "kafka.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "kafka.name" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.kafka.replicas | int }}
  {{- if not .Values.kafka.configmap.zookeeper_connect }}
  # Use OrderedReady for KRaft mode to ensure quorum forms properly
  podManagementPolicy: "OrderedReady"
  {{- else }}
  podManagementPolicy: {{ .Values.kafka.podManagementPolicy }}
  {{- end }}
  updateStrategy:
    type: {{ .Values.kafka.updateStrategy }}
    {{- if (eq "Recreate" .Values.kafka.updateStrategy) }}
    rollingUpdate: null
    {{- else if .Values.kafka.rollingUpdatePartition }}
    rollingUpdate:
      partition: {{ .Values.kafka.rollingUpdatePartition }}
    {{- end }}
  serviceName: {{ include "kafka.fullname" . }}-headless
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "kafka.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap-kafka.yaml") . | sha256sum }}
      {{- if and .Values.kafka.metrics.enabled .Values.kafka.metrics.podAnnotations }}
      {{- toYaml .Values.kafka.metrics.podAnnotations | nindent 8 }}
      {{- end }}
      {{- if .Values.kafka.podAnnotations }}
      {{- range $key, $value := .Values.kafka.podAnnotations }}
        {{ $key }}: {{ $value | quote }}
      {{- end }}
      {{- end }}
      labels:
        app.kubernetes.io/name: {{ include "kafka.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
      {{- if and .Values.kafka.metrics.enabled .Values.kafka.metrics.podLabels }}
      {{- toYaml .Values.kafka.metrics.podLabels | nindent 8 }}
      {{- end }}
      {{- if .Values.kafka.podLabels }}
      {{- range $key, $value := .Values.kafka.podLabels }}
        {{ $key }}: {{ $value | quote }}
      {{- end }}
      {{- end }}
    spec:
    {{- with .Values.kafka.priorityClassName }}
      priorityClassName: {{ . }}
    {{- end }}
    {{- with .Values.kafka.podSecurityContext }}
      securityContext:
        {{- toYaml . | nindent 8 }}
    {{- end }}
      # Init containers
      initContainers:
      # Fix permissions for /opt/kafka/logs
      - name: fix-kafka-logs-permissions
        image: busybox:latest
        command: ['sh', '-c']
        args:
          - |
            mkdir -p /opt/kafka/logs
            chown -R 1001:1001 /opt/kafka/logs || true
            chmod -R 755 /opt/kafka/logs || true
        securityContext:
          runAsUser: 0
        volumeMounts:
        - name: kafka-logs
          mountPath: /opt/kafka/logs
      {{- if not .Values.kafka.configmap.zookeeper_connect }}
      # Format storage for KRaft mode (Bitnami-style approach)
      - name: format-kafka-storage
        image: {{ .Values.kafka.image }}:{{ .Values.kafka.imageVersion | default .Chart.AppVersion }}
        imagePullPolicy: {{ .Values.kafka.imagePullPolicy }}
        command: ['/bin/bash', '-c']
        args:
          - |
            export POD_ORDINAL=${HOSTNAME##*-}
            export BROKER_ID=${POD_ORDINAL}
            export LOG_DIRS={{ include "kafka.fullpath" . }}
            # Ensure log directory exists
            mkdir -p "${LOG_DIRS}"
            # Remove lost+found directory if it exists
            rm -rf "${LOG_DIRS}/lost+found" 2>/dev/null || true
            # Check if storage is already formatted
            if [ ! -f "${LOG_DIRS}/meta.properties" ]; then
              echo "Formatting Kafka storage for KRaft mode (broker ${BROKER_ID})..."
              # Generate or use cluster ID
              CLUSTER_ID="{{ .Values.kafka.configmap.cluster_id }}"
              if [ -z "$CLUSTER_ID" ]; then
                # Generate cluster ID from release name (deterministic, same for all pods)
                CLUSTER_ID=$(echo -n "{{ .Release.Name }}-{{ .Release.Namespace }}" | sha256sum | cut -d' ' -f1 | cut -c1-16)
              fi
              # Create minimal config for storage formatting
              # Must include all voters so validation passes for each broker
              # Build config file line by line to avoid heredoc YAML parsing issues
              {
                echo "broker.id=\${BROKER_ID}"
                echo "node.id=\${BROKER_ID}"
                echo "process.roles=broker,controller"
                echo "controller.listener.names=CONTROLLER"
                echo "listeners=CONTROLLER://0.0.0.0:{{ .Values.kafka.configmap.controller_listener_port | default "9094" }}"
                echo "log.dirs=\${LOG_DIRS}"
                echo "controller.quorum.voters={{- range $i := until (.Values.kafka.replicas | int) }}{{ if $i }},{{ end }}{{ $i }}@{{ include "kafka.fullname" $ }}-{{ $i }}.{{ include "kafka.fullname" $ }}-headless.{{ $.Release.Namespace }}.svc.{{ $.Values.clusterDomain }}:{{ $.Values.kafka.configmap.controller_listener_port | default "9094" }}{{ end }}"
              } > /tmp/storage-format.properties
              # Substitute shell variables in the config file (Helm variables already expanded)
              sed -i "s|\${BROKER_ID}|${BROKER_ID}|g" /tmp/storage-format.properties
              sed -i "s|\${LOG_DIRS}|${LOG_DIRS}|g" /tmp/storage-format.properties
              # Format storage
              /opt/kafka/bin/kafka-storage.sh format -t "$CLUSTER_ID" -c /tmp/storage-format.properties --ignore-formatted
              echo "Storage formatted with cluster ID: $CLUSTER_ID for node ID: $BROKER_ID"
            else
              echo "Storage already formatted, skipping format step"
            fi
        securityContext:
          runAsUser: 1001
          runAsGroup: 1001
        volumeMounts:
        - name: {{ include "kafka.fullname" . }}-data
          mountPath: {{ include "kafka.fullpath" . }}
      {{- end }}
      volumes:
      - name: kafka-logs
        emptyDir: {}
    {{- if .Values.affinity }}
      affinity:
{{ toYaml .Values.affinity | indent 8 }}
    {{- end }}
    {{- if .Values.tolerations }}
      tolerations:
{{ toYaml .Values.tolerations | indent 8 }}
    {{- end }}
    {{- if .Values.kafka.imagePullSecrets }}
      imagePullSecrets:
    {{- range .Values.kafka.imagePullSecrets }}
      - name: {{ . | quote }}
    {{- end }}
    {{- end }}
      containers:
      - name: {{ include "kafka.fullname" . }}
        image: {{ .Values.kafka.image }}:{{ .Values.kafka.imageVersion | default .Chart.AppVersion }}
        imagePullPolicy: {{ .Values.kafka.imagePullPolicy }}
        command:
          - /bin/bash
          - -c
          - |
            export POD_ORDINAL=${HOSTNAME##*-}
            export BROKER_ID=${POD_ORDINAL}
            export LOG_DIRS={{ include "kafka.fullpath" . }}
            # Ensure log directory exists (fsGroup in podSecurityContext handles permissions)
            mkdir -p "${LOG_DIRS}"
            # Remove lost+found directory if it exists (created by ext filesystems)
            # Kafka doesn't allow any directories that aren't topic-partition format
            rm -rf "${LOG_DIRS}/lost+found" 2>/dev/null || true
            # /opt/kafka/logs is now writable via initContainer, so GC logging should work
            # Copy config to writable location and substitute variables
            cp /opt/kafka/config/server.properties /tmp/server.properties
            sed -i "s/\${POD_ORDINAL}/${POD_ORDINAL}/g" /tmp/server.properties
            sed -i "s/\${KAFKA_BROKER_ID}/${BROKER_ID}/g" /tmp/server.properties
            sed -i "s|\${KAFKA_ADVERTISED_LISTENERS}|PLAINTEXT://{{ include "kafka.fullname" . }}-${POD_ORDINAL}.{{ include "kafka.fullname" . }}-headless.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}:{{ .Values.kafka.client_port }},PLAINTEXT_INTERNAL://{{ include "kafka.fullname" . }}-${POD_ORDINAL}.{{ include "kafka.fullname" . }}-headless.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}:{{ .Values.kafka.internal_port }}|g" /tmp/server.properties
            sed -i "s|\${KAFKA_LISTENERS}|{{ include "kafka.listeners" . }}|g" /tmp/server.properties
            sed -i "s|\${KAFKA_LOG_DIRS}|${LOG_DIRS}|g" /tmp/server.properties
            sed -i "s|\${KAFKA_NUM_PARTITIONS}|{{ .Values.kafka.configmap.num_partitions }}|g" /tmp/server.properties
            sed -i "s|\${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}|{{ .Values.kafka.configmap.offsets_topic_replication_factor }}|g" /tmp/server.properties
            sed -i "s|\${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR}|{{ .Values.kafka.configmap.transaction_state_log_replication_factor }}|g" /tmp/server.properties
            sed -i "s|\${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR}|{{ .Values.kafka.configmap.transaction_state_log_min_isr }}|g" /tmp/server.properties
            sed -i "s|\${KAFKA_LOG_RETENTION_HOURS}|{{ .Values.kafka.configmap.log_retention_hours }}|g" /tmp/server.properties
            sed -i "s|\${KAFKA_LOG_SEGMENT_BYTES}|{{ .Values.kafka.configmap.log_segment_bytes }}|g" /tmp/server.properties
            {{- if not .Values.kafka.configmap.zookeeper_connect }}
            # For non-broker-0 pods, wait a bit to ensure broker 0 has time to initialize
            if [ "$BROKER_ID" != "0" ]; then
              echo "Waiting for broker 0 to initialize before starting broker $BROKER_ID..."
              # Give broker 0 time to start and form the initial quorum
              # With OrderedReady, broker 0 should be running, but may still be initializing
              sleep 30
              echo "Starting broker $BROKER_ID..."
            fi
            {{- end }}
            # Start Kafka (storage is already formatted by init container)
            /opt/kafka/bin/kafka-server-start.sh /tmp/server.properties
        env:
        - name: POD_ORDINAL
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: KAFKA_JVM_PERFORMANCE_OPTS
          value: ""
        ports:
        - name: client-port
          containerPort: {{ .Values.kafka.client_port | int }}
          protocol: TCP
        - name: internal-port
          containerPort: {{ .Values.kafka.internal_port | int }}
          protocol: TCP
        {{- if .Values.kafka.metrics.enabled }}
        - name: metrics
          containerPort: {{ .Values.kafka.metrics.port }}
          protocol: TCP
        {{- end }}
      {{- if .Values.kafka.startupProbe.enabled }}
        startupProbe:
          tcpSocket:
            port: client-port
          {{- if .Values.kafka.startupProbe.initialDelaySeconds }}
          initialDelaySeconds: {{ .Values.kafka.startupProbe.initialDelaySeconds }}
          {{- end }}
          periodSeconds: {{ .Values.kafka.startupProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.kafka.startupProbe.timeoutSeconds }}
          failureThreshold: {{ .Values.kafka.startupProbe.failureThreshold }}
          successThreshold: {{ .Values.kafka.startupProbe.successThreshold }}
      {{- end }}
      {{- if .Values.kafka.livenessProbe.enabled }}
        livenessProbe:
          tcpSocket:
            port: client-port
          initialDelaySeconds: {{ .Values.kafka.livenessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.kafka.livenessProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.kafka.livenessProbe.timeoutSeconds }}
          failureThreshold: {{ .Values.kafka.livenessProbe.failureThreshold }}
          successThreshold: {{ .Values.kafka.livenessProbe.successThreshold }}
      {{- end }}
      {{- if .Values.kafka.readinessProbe.enabled }}
        readinessProbe:
          tcpSocket:
            port: client-port
          initialDelaySeconds: {{ .Values.kafka.readinessProbe.initialDelaySeconds }}
          periodSeconds: {{ .Values.kafka.readinessProbe.periodSeconds }}
          timeoutSeconds: {{ .Values.kafka.readinessProbe.timeoutSeconds }}
          failureThreshold: {{ .Values.kafka.readinessProbe.failureThreshold }}
          successThreshold: {{ .Values.kafka.readinessProbe.successThreshold }}
      {{- end }}
      {{- if .Values.kafka.resources }}
        {{- $resources := dict }}
        {{- if .Values.kafka.resources.limits }}
          {{- $limits := dict }}
          {{- if and .Values.kafka.resources.limits.cpu (ne .Values.kafka.resources.limits.cpu "") }}
            {{- $_ := set $limits "cpu" .Values.kafka.resources.limits.cpu }}
          {{- end }}
          {{- if and .Values.kafka.resources.limits.memory (ne .Values.kafka.resources.limits.memory "") }}
            {{- $_ := set $limits "memory" .Values.kafka.resources.limits.memory }}
          {{- end }}
          {{- if $limits }}
            {{- $_ := set $resources "limits" $limits }}
          {{- end }}
        {{- end }}
        {{- if .Values.kafka.resources.requests }}
          {{- $requests := dict }}
          {{- if and .Values.kafka.resources.requests.cpu (ne .Values.kafka.resources.requests.cpu "") }}
            {{- $_ := set $requests "cpu" .Values.kafka.resources.requests.cpu }}
          {{- end }}
          {{- if and .Values.kafka.resources.requests.memory (ne .Values.kafka.resources.requests.memory "") }}
            {{- $_ := set $requests "memory" .Values.kafka.resources.requests.memory }}
          {{- end }}
          {{- if $requests }}
            {{- $_ := set $resources "requests" $requests }}
          {{- end }}
        {{- end }}
        {{- if $resources }}
        resources:
{{ toYaml $resources | indent 10 }}
        {{- end }}
      {{- end }}
        volumeMounts:
        - name: {{ include "kafka.fullname" . }}-data
          mountPath: {{ include "kafka.fullpath" . }}
        - name: kafka-logs
          mountPath: /opt/kafka/logs
        {{- if .Values.kafka.configmap.enabled }}
        - name: {{ include "kafka.fullname" . }}-config
          mountPath: /opt/kafka/config
        {{- end }}
{{- if .Values.kafka.volumeMounts }}
{{ toYaml .Values.kafka.volumeMounts | indent 8 }}
{{- end }}
      {{- with .Values.kafka.securityContext }}
        securityContext:
          {{- toYaml . | nindent 10 }}
      {{- end }}
    {{- if .Values.kafka.nodeSelector }}
      nodeSelector:
{{ toYaml .Values.kafka.nodeSelector | indent 8 }}
    {{- end }}
      volumes:
      - name: {{ include "kafka.fullname" . }}-data
      {{- if .Values.kafka.persistentVolumeClaim.dataPersistentVolume.enabled }}
        persistentVolumeClaim:
          claimName: {{ include "kafka.fullname" . }}-data
      {{- else }}
        emptyDir: {}
      {{- end }}
      - name: kafka-logs
        emptyDir: {}
      {{- if .Values.kafka.configmap.enabled }}
      - name: {{ include "kafka.fullname" . }}-config
        configMap:
          name: {{ include "kafka.fullname" . }}-config
          items:
          - key: server.properties
            path: server.properties
          {{- if .Values.kafka.configmap.configOverride }}
          - key: override.properties
            path: override.properties
          {{- end }}
      {{- end }}
{{- if .Values.kafka.volumes }}
{{ toYaml .Values.kafka.volumes | indent 6 }}
{{- end }}
      {{- if .Values.serviceAccount.enabled }}
      serviceAccountName: {{ .Values.serviceAccount.name }}
      {{- end }}
  {{- if .Values.kafka.persistentVolumeClaim.enabled }}
  volumeClaimTemplates:
  {{- if .Values.kafka.persistentVolumeClaim.dataPersistentVolume.enabled }}
  - metadata:
      name: {{ include "kafka.fullname" . }}-data
      labels:
        app.kubernetes.io/name: {{ include "kafka.name" . }}-data
        app.kubernetes.io/instance: {{ .Release.Name }}-data
        app.kubernetes.io/managed-by: {{ .Release.Service }}
    spec:
      accessModes:
      {{- range .Values.kafka.persistentVolumeClaim.dataPersistentVolume.accessModes }}
      - {{ . | quote }}
      {{- end }}
      {{- if .Values.kafka.persistentVolumeClaim.dataPersistentVolume.storageClassName }}
      storageClassName: {{ .Values.kafka.persistentVolumeClaim.dataPersistentVolume.storageClassName | quote }}
      {{- end }}
      resources:
        requests:
          storage: {{ .Values.kafka.persistentVolumeClaim.dataPersistentVolume.storage | quote }}
  {{- end }}
  {{- end }}

